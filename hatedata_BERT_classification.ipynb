{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\david\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='C:\\\\Users\\\\david\\\\Desktop\\\\대학원\\\\Individual_project\\\\mbti_project\\\\Hatespeech_data\\\\hatespeech_sep'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_data</th>\n",
       "      <th>prepro_text_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The reason is sexism. MENTION1773 Female comed...</td>\n",
       "      <td>the reason is sexism female comedians just are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>â­â­â­â­â­ review by robe gardner: \"aweso...</td>\n",
       "      <td>review by robe gardner awesome product looks g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>@user  in chief- the gangs all here #hillarycl...</td>\n",
       "      <td>in chief the gangs all here hillaryclinton oba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>MENTION963 killing you how?</td>\n",
       "      <td>killing you how</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@user @user the correct way it should have bee...</td>\n",
       "      <td>the correct way it should have been written do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9508</th>\n",
       "      <td>0</td>\n",
       "      <td>got tickets to the great moscow state circus. ...</td>\n",
       "      <td>got tickets to the great moscow state circus d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9509</th>\n",
       "      <td>0</td>\n",
       "      <td>cups of tea and fresh mags.   #days @user in o...</td>\n",
       "      <td>cups of tea and fresh mags days in our stirrup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9510</th>\n",
       "      <td>1</td>\n",
       "      <td>MENTION3489 i dont expect that from a woman. I...</td>\n",
       "      <td>i dont expect that from a woman i expected wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9511</th>\n",
       "      <td>1</td>\n",
       "      <td>@user #americans suppo these  #zionist #occupi...</td>\n",
       "      <td>americans suppo these zionist occupiers let s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9512</th>\n",
       "      <td>1</td>\n",
       "      <td>when a girl tells a joke that's not funny at a...</td>\n",
       "      <td>when a girl tells a joke that s not funny at a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9513 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                          text_data  \\\n",
       "0         1  The reason is sexism. MENTION1773 Female comed...   \n",
       "1         0  â­â­â­â­â­ review by robe gardner: \"aweso...   \n",
       "2         1  @user  in chief- the gangs all here #hillarycl...   \n",
       "3         0                        MENTION963 killing you how?   \n",
       "4         1  @user @user the correct way it should have bee...   \n",
       "...     ...                                                ...   \n",
       "9508      0  got tickets to the great moscow state circus. ...   \n",
       "9509      0  cups of tea and fresh mags.   #days @user in o...   \n",
       "9510      1  MENTION3489 i dont expect that from a woman. I...   \n",
       "9511      1  @user #americans suppo these  #zionist #occupi...   \n",
       "9512      1  when a girl tells a joke that's not funny at a...   \n",
       "\n",
       "                                          prepro_text_2  \n",
       "0     the reason is sexism female comedians just are...  \n",
       "1     review by robe gardner awesome product looks g...  \n",
       "2     in chief the gangs all here hillaryclinton oba...  \n",
       "3                                       killing you how  \n",
       "4     the correct way it should have been written do...  \n",
       "...                                                 ...  \n",
       "9508  got tickets to the great moscow state circus d...  \n",
       "9509  cups of tea and fresh mags days in our stirrup...  \n",
       "9510  i dont expect that from a woman i expected wom...  \n",
       "9511  americans suppo these zionist occupiers let s ...  \n",
       "9512  when a girl tells a joke that s not funny at a...  \n",
       "\n",
       "[9513 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_model용.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 토큰화 함수 정의\n",
    "def tokenize_and_format(sentences):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # 문장을 인코딩\n",
    "                            add_special_tokens = True, # '[CLS]'와 '[SEP]' 추가\n",
    "                            max_length = 64,           # 문장의 최대 길이 설정\n",
    "                            pad_to_max_length = True,  # 패딩 적용\n",
    "                            return_attention_mask = True, # 어텐션 마스크 생성\n",
    "                            return_tensors = 'pt',     # 파이토치 텐서로 반환\n",
    "                     )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터와 레이블 분리\n",
    "X = df['prepro_text_2']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터를 훈련+검증 세트와 테스트 세트로 분할\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 훈련+검증 세트를 훈련 세트와 검증 세트로 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)  # 0.25 * 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련, 검증, 테스트 데이터셋을 토큰화\n",
    "train_inputs, train_masks = tokenize_and_format(X_train)\n",
    "val_inputs, val_masks = tokenize_and_format(X_val)\n",
    "test_inputs, test_masks = tokenize_and_format(X_test)\n",
    "\n",
    "# 레이블을 텐서로 변환\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "val_labels = torch.tensor(y_val.values)\n",
    "test_labels = torch.tensor(y_test.values)\n",
    "\n",
    "# TensorDataset 객체 생성\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈 설정\n",
    "batch_size = 16\n",
    "\n",
    "# 훈련 데이터 로더 생성\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # 훈련 데이터셋\n",
    "            sampler = RandomSampler(train_dataset), # 데이터셋에서 무작위로 샘플링\n",
    "            batch_size = batch_size # 배치 사이즈\n",
    "        )\n",
    "\n",
    "# 검증 데이터 로더 생성\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # 검증 데이터셋\n",
    "            sampler = SequentialSampler(val_dataset), # 순차적 샘플링\n",
    "            batch_size = batch_size # 배치 사이즈\n",
    "        )\n",
    "\n",
    "# 테스트 데이터 로더 생성\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # 테스트 데이터셋\n",
    "            sampler = SequentialSampler(test_dataset), # 순차적 샘플링\n",
    "            batch_size = batch_size # 배치 사이즈\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # 소문자로 된 BERT 베이스 모델 사용\n",
    "    num_labels = 2, # 이진 분류를 위한 레이블 수\n",
    "    output_attentions = False, # 어텐션 가중치를 반환할지 여부\n",
    "    output_hidden_states = False, # 히든 상태를 반환할지 여부\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 알고리즘 설정\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, # 학습률\n",
    "                  eps = 1e-8 # 수치 안정성을 위한 작은 값\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   1%|          | 3/357 [00:05<11:22,  1.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 역전파\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 그래디언트 클리핑\u001b[39;00m\n\u001b[0;32m     42\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 장치 설정 (GPU 사용 가능한 경우 GPU 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 에포크 수\n",
    "epochs = 10\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 훈련 모드로 설정\n",
    "    model.train()\n",
    "    \n",
    "    # `tqdm`으로 훈련 데이터 로더 감싸기\n",
    "    total_train_loss = 0\n",
    "    train_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    \n",
    "    for step, batch in enumerate(train_iterator):\n",
    "        # 배치를 GPU에 로드\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()        \n",
    "        \n",
    "        # 순전파\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        # 손실 값\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        \n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # 가중치 업데이트\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 평균 훈련 손실 계산\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    # 검증 세트 평가\n",
    "    print(\"\\nValidation...\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = b_input_mask.to(device)\n",
    "        b_labels = b_labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += accuracy_score(label_ids, np.argmax(logits, axis=1))\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\"Epoch: {}, Validation Accuracy: {:.2f}, Validation Loss: {:.2f}\".format(epoch_i+1, avg_val_accuracy, avg_val_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
