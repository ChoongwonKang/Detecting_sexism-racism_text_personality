{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Big5 traits through BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\david\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup, BertConfig, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\david\\\\Desktop\\\\대학원\\\\Individual_project\\\\mbti_project\\\\MBTI&BigFive_data\\\\전처리데이터\\\\BigFive'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.raw_data = \"BigFive_prepro_sen.csv\"  # 파일 확장자를 .csv로 변경\n",
    "        self.max_len = 64\n",
    "        self.batch_size = 16\n",
    "        self.num_labels = 2\n",
    "        self.epochs = 10\n",
    "        self.seed_val = 42\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def load_data(file_path):\n",
    "    # CSV 파일 로드\n",
    "    file_path = \"BigFive_prepro_sen.csv\"\n",
    "    temp = pd.read_csv(file_path, sep=\",\")\n",
    "    # 'cleaned_text' 열을 텍스트 데이터로 사용\n",
    "    document = temp['cleaned_text'].tolist()\n",
    "    # 'cEXT' 열의 값에 따라 labels 변수를 0 또는 1로 변환\n",
    "    labels = temp['cCON'].apply(lambda x: 1 if x == 'y' else 0).tolist()\n",
    "    return document, labels\n",
    "\n",
    "\n",
    "def add_special_token(document):\n",
    "    # 문장으로 분할하고 [CLS], [SEP] 토큰을 추가하는 과정\n",
    "    processed_docs = []\n",
    "    for doc in document:\n",
    "        sentences = re.split(r'[.!?]\\s+', doc)  # 문장 분할\n",
    "        processed_doc = \"[CLS] \" + \" [SEP] \".join(sentences) + \" [SEP]\"\n",
    "        processed_docs.append(processed_doc)\n",
    "    return processed_docs\n",
    "\n",
    "\n",
    "def tokenization(document):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokenized = [tokenizer.tokenize(sentence) for sentence in tqdm(document, desc=\"Tokenizing\")]\n",
    "    ids = [tokenizer.convert_tokens_to_ids(sentence) for sentence in tokenized]\n",
    "    return ids\n",
    "\n",
    "def padding(ids, args):\n",
    "    ids = pad_sequences(ids, args.max_len, dtype=\"long\", truncating='post', padding='post')\n",
    "    return ids\n",
    "\n",
    "\n",
    "# 학습 속도를 높이기 위한 어텐션 마스크 표시\n",
    "def attention_mask(ids):\n",
    "    masks = []\n",
    "    for id in ids:\n",
    "        mask = [float(i>0) for i in id]\n",
    "        masks.append(mask)\n",
    "    return masks\n",
    "\n",
    "\n",
    "def preprocess(args):\n",
    "    document, labels = load_data(args.raw_data)\n",
    "    document = add_special_token(document)  # 문장 분할 및 특수 토큰 추가\n",
    "    ids = tokenization(document)\n",
    "    ids = padding(ids, args)\n",
    "    masks = attention_mask(ids)\n",
    "    del document\n",
    "    return ids, masks, labels\n",
    "\n",
    "\n",
    "def train_test_data_split(ids, masks, labels):\n",
    "    train_ids, test_ids, train_labels, test_labels = train_test_split(ids, labels, random_state=42, test_size=0.2, stratify=labels)\n",
    "    train_masks, test_masks, _, _ = train_test_split(masks, ids, random_state=42, test_size=0.2, stratify=labels)\n",
    "    return train_ids, train_masks, train_labels, test_ids, test_masks, test_labels\n",
    "\n",
    "\n",
    "def build_dataloader(ids, masks, label, args):\n",
    "    dataloader = TensorDataset(torch.tensor(ids), torch.tensor(masks), torch.tensor(label))\n",
    "    dataloader = DataLoader(dataloader, sampler=RandomSampler(dataloader), batch_size=args.batch_size)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def build_model(args):\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=args.num_labels)\n",
    "    # CUDA가 사용 가능한지 확인하고, 그렇지 않으면 CPU 사용\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"{torch.cuda.get_device_name(0)} available\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. Using CPU instead.\")\n",
    "    model = model.to(device)\n",
    "    return model, device\n",
    "\n",
    "\n",
    "def test(test_dataloader, model, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(index.to(device) for index in batch)\n",
    "        ids, masks, labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids, token_type_ids=None, attention_mask=masks)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_true.extend(labels)\n",
    "    accuracy = accuracy_score(all_true, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_true, all_preds, average='binary')\n",
    "    \n",
    "    print(f\"Test Average Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    # Return all metrics for further use if needed\n",
    "    return accuracy, precision, recall, f1\n",
    "   \n",
    "   \n",
    "# 수정된 train 함수\n",
    "def train(train_dataloader, test_dataloader, args):\n",
    "    model, device = build_model(args)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8, weight_decay=0.01) #L2 Reg\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*args.epochs)\n",
    "    random.seed(args.seed_val)\n",
    "    np.random.seed(args.seed_val)\n",
    "    torch.manual_seed(args.seed_val)\n",
    "    torch.cuda.manual_seed_all(args.seed_val)\n",
    "    model.zero_grad()\n",
    "    \n",
    "    for epoch in range(args.epochs):  # 수정: range(0, args.epochs) -> range(args.epochs)\n",
    "        model.train()\n",
    "        total_loss, total_accuracy = 0, 0\n",
    "        print(\"-\"*30)\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\", leave=False)):\n",
    "            if step % 139 == 0:\n",
    "                print(f\"Epoch : {epoch+1} / {args.epochs}, Step : {step}\")\n",
    "            batch = tuple(index.to(device) for index in batch)\n",
    "            ids, masks, labels = batch\n",
    "            outputs = model(ids, token_type_ids=None, attention_mask=masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            pred = [torch.argmax(logit).cpu().detach().item() for logit in outputs.logits]\n",
    "            true = [label for label in labels.cpu().numpy()]\n",
    "            accuracy = accuracy_score(true, pred)\n",
    "            total_accuracy += accuracy\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        avg_accuracy = total_accuracy / len(train_dataloader)\n",
    "        print(f\"{epoch+1} Epoch Average train loss: {avg_loss}\")\n",
    "        print(f\"{epoch+1} Epoch Average train accuracy: {avg_accuracy}\")\n",
    "        acc, precision, recall, f1 = test(test_dataloader, model, device)\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        f = os.path.join(\"results\", f'epoch_{epoch+1}_evalAcc_{acc*100:.0f}.pth')\n",
    "        torch.save(model.state_dict(), f)\n",
    "        print('Saved checkpoint:', f)\n",
    "\n",
    "\n",
    "# 수정된 run 함수\n",
    "def run(args):\n",
    "    ids, masks, labels = preprocess(args)\n",
    "    train_ids, train_masks, train_labels, test_ids, test_masks, test_labels = train_test_data_split(ids, masks, labels)\n",
    "    train_dataloader = build_dataloader(train_ids, train_masks, train_labels, args)\n",
    "    test_dataloader = build_dataloader(test_ids, test_masks, test_labels, args)\n",
    "    train(train_dataloader, test_dataloader, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 2467/2467 [00:15<00:00, 161.57it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU instead.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 / 10, Step : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Epoch Average train loss: 0.6978807819466437\n",
      "1 Epoch Average train accuracy: 0.5089717741935483\n",
      "Test Average Accuracy: 0.54\n",
      "Precision: 0.54\n",
      "Recall: 1.00\n",
      "F1 Score: 0.70\n",
      "Saved checkpoint: results\\epoch_1_evalAcc_54.pth\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 / 10, Step : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Epoch Average train loss: 0.6846251300265712\n",
      "2 Epoch Average train accuracy: 0.5608870967741936\n",
      "Test Average Accuracy: 0.56\n",
      "Precision: 0.57\n",
      "Recall: 0.75\n",
      "F1 Score: 0.65\n",
      "Saved checkpoint: results\\epoch_2_evalAcc_56.pth\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 / 10, Step : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Epoch Average train loss: 0.6296449137791511\n",
      "3 Epoch Average train accuracy: 0.651108870967742\n",
      "Test Average Accuracy: 0.56\n",
      "Precision: 0.67\n",
      "Recall: 0.36\n",
      "F1 Score: 0.47\n",
      "Saved checkpoint: results\\epoch_3_evalAcc_56.pth\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 / 10, Step : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Epoch Average train loss: 0.4465491251359063\n",
      "4 Epoch Average train accuracy: 0.7932459677419355\n",
      "Test Average Accuracy: 0.54\n",
      "Precision: 0.56\n",
      "Recall: 0.72\n",
      "F1 Score: 0.63\n",
      "Saved checkpoint: results\\epoch_4_evalAcc_54.pth\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 / 10, Step : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Epoch Average train loss: 0.22489440224824414\n",
      "5 Epoch Average train accuracy: 0.9163306451612904\n",
      "Test Average Accuracy: 0.56\n",
      "Precision: 0.56\n",
      "Recall: 0.82\n",
      "F1 Score: 0.67\n",
      "Saved checkpoint: results\\epoch_5_evalAcc_56.pth\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 / 10, Step : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Epoch Average train loss: 0.0994249229627331\n",
      "6 Epoch Average train accuracy: 0.9647177419354839\n",
      "Test Average Accuracy: 0.56\n",
      "Precision: 0.58\n",
      "Recall: 0.70\n",
      "F1 Score: 0.63\n",
      "Saved checkpoint: results\\epoch_6_evalAcc_56.pth\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7 / 10, Step : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Epoch Average train loss: 0.04845293859327813\n",
      "7 Epoch Average train accuracy: 0.985383064516129\n",
      "Test Average Accuracy: 0.57\n",
      "Precision: 0.58\n",
      "Recall: 0.73\n",
      "F1 Score: 0.65\n",
      "Saved checkpoint: results\\epoch_7_evalAcc_57.pth\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8 / 10, Step : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Epoch Average train loss: 0.03331322323825706\n",
      "8 Epoch Average train accuracy: 0.9924395161290323\n",
      "Test Average Accuracy: 0.57\n",
      "Precision: 0.59\n",
      "Recall: 0.72\n",
      "F1 Score: 0.64\n",
      "Saved checkpoint: results\\epoch_8_evalAcc_57.pth\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9 / 10, Step : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Epoch Average train loss: 0.006923062686954865\n",
      "9 Epoch Average train accuracy: 0.9974798387096774\n",
      "Test Average Accuracy: 0.57\n",
      "Precision: 0.60\n",
      "Recall: 0.63\n",
      "F1 Score: 0.62\n",
      "Saved checkpoint: results\\epoch_9_evalAcc_57.pth\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10 / 10, Step : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Epoch Average train loss: 0.007790874487498866\n",
      "10 Epoch Average train accuracy: 0.9984879032258065\n",
      "Test Average Accuracy: 0.57\n",
      "Precision: 0.59\n",
      "Recall: 0.68\n",
      "F1 Score: 0.63\n",
      "Saved checkpoint: results\\epoch_10_evalAcc_57.pth\n"
     ]
    }
   ],
   "source": [
    "#Action\n",
    "run(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
