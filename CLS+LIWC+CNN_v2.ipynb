{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\david\\\\Desktop\\\\대학원\\\\Individual_project\\\\mbti_project\\\\MBTI&BigFive_data\\\\전처리데이터\\\\240322시험')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_test_split (8:2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset and features have been created and saved.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"MBTI_reprepro.csv\")\n",
    "features = np.load(\"MBTICLS.npy\")\n",
    "\n",
    "# 데이터를 8:2로 분할, stratify를 적용하여 'label' 열 기준으로 분할\n",
    "df_train, df_test, features_train, features_test = train_test_split(\n",
    "    df, features, test_size=0.2, stratify=df['type'], random_state=42)\n",
    "\n",
    "# 분할된 데이터셋과 특성 저장\n",
    "df_train.to_csv(\"MBTI_train.csv\", index=False, encoding = 'utf-8-sig')\n",
    "df_test.to_csv(\"MBTI_test.csv\", index=False, encoding = 'utf-8-sig')\n",
    "np.save(\"MBTI_train.npy\", features_train)\n",
    "np.save(\"MBTI_test.npy\", features_test)\n",
    "\n",
    "print(\"Test dataset and features have been created and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validate (8:2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.7896, Train Acc: 0.4882, Val Loss: 0.6891, Val Acc: 0.5296\n",
      "Epoch 2/30, Train Loss: 0.7494, Train Acc: 0.5126, Val Loss: 0.6840, Val Acc: 0.5592\n",
      "Epoch 3/30, Train Loss: 0.7454, Train Acc: 0.5292, Val Loss: 0.6978, Val Acc: 0.4970\n",
      "Epoch 4/30, Train Loss: 0.7219, Train Acc: 0.5473, Val Loss: 0.6726, Val Acc: 0.6036\n",
      "Epoch 5/30, Train Loss: 0.7055, Train Acc: 0.5558, Val Loss: 0.6646, Val Acc: 0.5947\n",
      "Epoch 6/30, Train Loss: 0.7163, Train Acc: 0.5514, Val Loss: 0.6674, Val Acc: 0.6139\n",
      "Epoch 7/30, Train Loss: 0.6941, Train Acc: 0.5828, Val Loss: 0.6801, Val Acc: 0.5740\n",
      "Epoch 8/30, Train Loss: 0.6882, Train Acc: 0.5891, Val Loss: 0.6928, Val Acc: 0.5340\n",
      "Epoch 9/30, Train Loss: 0.6906, Train Acc: 0.5825, Val Loss: 0.6612, Val Acc: 0.6154\n",
      "Epoch 10/30, Train Loss: 0.6925, Train Acc: 0.5821, Val Loss: 0.6652, Val Acc: 0.6243\n",
      "Epoch 11/30, Train Loss: 0.6872, Train Acc: 0.5869, Val Loss: 0.6621, Val Acc: 0.6228\n",
      "Epoch 12/30, Train Loss: 0.6838, Train Acc: 0.5828, Val Loss: 0.6728, Val Acc: 0.5991\n",
      "Epoch 13/30, Train Loss: 0.6808, Train Acc: 0.5995, Val Loss: 0.6583, Val Acc: 0.5976\n",
      "Epoch 14/30, Train Loss: 0.6687, Train Acc: 0.6047, Val Loss: 0.6571, Val Acc: 0.5932\n",
      "Epoch 15/30, Train Loss: 0.6691, Train Acc: 0.6043, Val Loss: 0.6730, Val Acc: 0.5695\n",
      "Epoch 16/30, Train Loss: 0.6740, Train Acc: 0.6069, Val Loss: 0.6800, Val Acc: 0.5651\n",
      "Epoch 17/30, Train Loss: 0.6651, Train Acc: 0.6080, Val Loss: 0.6541, Val Acc: 0.6169\n",
      "Epoch 18/30, Train Loss: 0.6561, Train Acc: 0.6165, Val Loss: 0.6607, Val Acc: 0.6331\n",
      "Epoch 19/30, Train Loss: 0.6559, Train Acc: 0.6135, Val Loss: 0.6601, Val Acc: 0.6169\n",
      "Epoch 20/30, Train Loss: 0.6555, Train Acc: 0.6154, Val Loss: 0.6577, Val Acc: 0.6331\n",
      "Epoch 21/30, Train Loss: 0.6490, Train Acc: 0.6302, Val Loss: 0.6592, Val Acc: 0.5828\n",
      "Epoch 22/30, Train Loss: 0.6447, Train Acc: 0.6243, Val Loss: 0.6877, Val Acc: 0.5607\n",
      "Epoch 23/30, Train Loss: 0.6392, Train Acc: 0.6305, Val Loss: 0.6575, Val Acc: 0.6346\n",
      "Epoch 24/30, Train Loss: 0.6419, Train Acc: 0.6339, Val Loss: 0.6650, Val Acc: 0.6154\n",
      "Epoch 25/30, Train Loss: 0.6432, Train Acc: 0.6287, Val Loss: 0.6559, Val Acc: 0.6169\n",
      "Epoch 26/30, Train Loss: 0.6364, Train Acc: 0.6405, Val Loss: 0.6566, Val Acc: 0.6213\n",
      "Epoch 27/30, Train Loss: 0.6383, Train Acc: 0.6420, Val Loss: 0.6576, Val Acc: 0.6198\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, kernel_size=3, dropout_rate=0.5):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=kernel_size)\n",
    "        self.bn = nn.BatchNorm1d(64)  # Batch Normalization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=kernel_size)\n",
    "        # Dynamic calculation of FC layer input size\n",
    "        self.fc_input_size = self._calculate_fc_input_size(input_size, kernel_size)\n",
    "        self.fc = nn.Linear(self.fc_input_size, num_classes)\n",
    "    \n",
    "    def _calculate_fc_input_size(self, input_size, kernel_size):\n",
    "        size = input_size\n",
    "        size = (size - (kernel_size - 1) - 1) + 1  # Conv1d output size\n",
    "        size = size // kernel_size  # MaxPool1d output size\n",
    "        size = size * 64  # Considering the number of output channels from Conv1d\n",
    "        return size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def load_data(file_path, feature_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    labels = df['type'].apply(lambda x: 1 if 'P' in x else 0).tolist()\n",
    "    features = np.load(feature_path)\n",
    "    return labels, features\n",
    "\n",
    "def build_dataloader(X, y, batch_size):\n",
    "    tensor_x = torch.tensor(X).float()\n",
    "    tensor_y = torch.tensor(y).long()\n",
    "    dataset = TensorDataset(tensor_x, tensor_y)\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, device, epochs, early_stopping_patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, total_train = 0, 0, 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            train_correct += (pred == target).sum().item()\n",
    "            total_train += target.size(0)\n",
    "        \n",
    "        train_accuracy = train_correct / total_train\n",
    "        \n",
    "        val_loss, val_correct, total_val = 0, 0, 0\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "                _, pred = torch.max(output, dim=1)\n",
    "                val_correct += (pred == target).sum().item()\n",
    "                total_val += target.size(0)\n",
    "                \n",
    "        val_accuracy = val_correct / total_val\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def process_data_and_train_model(file_path, feature_path, batch_size, epochs, lr, device, seed=42):\n",
    "    set_seed(seed)  # Set the seed for reproducibility\n",
    "\n",
    "    # Load data\n",
    "    labels, features = load_data(file_path, feature_path)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=seed, stratify=labels)\n",
    "\n",
    "    # Create dataloaders for training and validation sets\n",
    "    train_loader = build_dataloader(X_train, y_train, batch_size)\n",
    "    val_loader = build_dataloader(X_val, y_val, batch_size)\n",
    "\n",
    "    # Model configuration\n",
    "    num_classes = 2\n",
    "    input_size = features.shape[1]  # Dynamic input size based on features\n",
    "    model = CNNClassifier(num_classes=num_classes, input_size=input_size).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)  # Added weight decay for regularization\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, device, epochs)\n",
    "\n",
    "# Assuming the paths to the dataset and features are provided correctly\n",
    "file_path = 'E&I_train.csv'\n",
    "feature_path = 'E&I_train.npy'\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "lr = 2e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#action\n",
    "process_data_and_train_model(file_path, feature_path, batch_size, epochs, lr, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5964, Precision: 0.5890, Recall: 0.4812, F1-Score: 0.5297\n"
     ]
    }
   ],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, kernel_size=3, dropout_rate=0.5):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=kernel_size)\n",
    "        self.bn = nn.BatchNorm1d(64)  # Batch Normalization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=kernel_size)\n",
    "        # Dynamic calculation of FC layer input size\n",
    "        self.fc_input_size = self._calculate_fc_input_size(input_size, kernel_size)\n",
    "        self.fc = nn.Linear(self.fc_input_size, num_classes)\n",
    "    \n",
    "    def _calculate_fc_input_size(self, input_size, kernel_size):\n",
    "        size = input_size\n",
    "        size = (size - (kernel_size - 1) - 1) + 1  # Conv1d output size\n",
    "        size = size // kernel_size  # MaxPool1d output size\n",
    "        size = size * 64  # Considering the number of output channels from Conv1d\n",
    "        return size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def load_data(file_path, feature_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    labels = df['type'].apply(lambda x: 1 if 'E' in x else 0).tolist()\n",
    "    features = np.load(feature_path)\n",
    "    return labels, features\n",
    "\n",
    "def build_dataloader(X, y, batch_size):\n",
    "    tensor_x = torch.tensor(X).float()\n",
    "    tensor_y = torch.tensor(y).long()\n",
    "    dataset = TensorDataset(tensor_x, tensor_y)\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "def load_best_model(model_path, num_classes, input_size, device):\n",
    "    model = CNNClassifier(num_classes=num_classes, input_size=input_size).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_correct, total = 0, 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            total_correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            all_predictions.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    accuracy = total_correct / total\n",
    "    precision = precision_score(all_targets, all_predictions)\n",
    "    recall = recall_score(all_targets, all_predictions)\n",
    "    f1 = f1_score(all_targets, all_predictions)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def test_model(test_file_path, test_feature_path, batch_size, model_path, device):\n",
    "    test_labels, test_features = load_data(test_file_path, test_feature_path)\n",
    "    test_loader = build_dataloader(test_features, test_labels, batch_size)\n",
    "    num_classes = 2\n",
    "    input_size = test_features.shape[1]\n",
    "\n",
    "    # Load the best model from training\n",
    "    best_model = load_best_model(model_path, num_classes, input_size, device)\n",
    "\n",
    "    # Evaluate the model on the test set with additional metrics\n",
    "    accuracy, precision, recall, f1 = evaluate(best_model, test_loader, device)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Assuming the paths to the test dataset and features, and the path to the best model are provided correctly\n",
    "test_file_path = 'E&I_test.csv'\n",
    "test_feature_path = \"E&I_test.npy\"\n",
    "model_path = \"best_model.pth\"\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Execute the testing of the model with the best validation weights\n",
    "test_model(test_file_path, test_feature_path, batch_size, model_path, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
