{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_csv('C:\\\\Users\\\\david\\\\Desktop\\\\대학원\\\\Individual_project\\\\mbti_project\\\\MBTI&BigFive_data\\\\전처리데이터\\\\MBTI_prepro_sen.csv')\n",
    "texts = df['cleaned_text'].tolist()\n",
    "labels = df['type'].tolist()\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class MBTIDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': torch.tensor(label)}\n",
    "    \n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "fold_results = []\n",
    "\n",
    "# Training loop with cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels_encoded)):\n",
    "    train_texts_fold = [texts[i] for i in train_idx]\n",
    "    val_texts_fold = [texts[i] for i in val_idx]\n",
    "    train_labels_fold = labels_encoded[train_idx]\n",
    "    val_labels_fold = labels_encoded[val_idx]\n",
    "\n",
    "    train_dataset = MBTIDataset(train_texts_fold, train_labels_fold, tokenizer)\n",
    "    val_dataset = MBTIDataset(val_texts_fold, val_labels_fold, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "    # Model and optimizer\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_encoder.classes_))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    for epoch in range(10):  # Example: 10 epochs\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch}, Fold {fold}'):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        precision, recall, f1, accuracy = evaluate_model(model, val_loader, device)\n",
    "        fold_results.append((precision, recall, f1, accuracy))\n",
    "        print(f'Fold {fold}, Epoch {epoch} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Fold', 'Epoch', 'Precision', 'Recall', 'F1-Score', 'Accuracy']\n",
    "results_df = pd.DataFrame(fold_results, columns=columns)\n",
    "\n",
    "# 성능 지표를 CSV 파일로 저장\n",
    "results_df.to_csv('model_performance.csv', index=False)\n",
    "print(\"성능 지표가 model_performance.csv 파일로 저장되었습니다.\")\n",
    "\n",
    "# 마지막 폴드의 모델과 토크나이저 저장\n",
    "model_save_path = \"final_roberta_model.pt\"\n",
    "tokenizer_save_path = \"final_roberta_tokenizer\"\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"모델이 {model_save_path} 경로에 저장되었습니다.\")\n",
    "\n",
    "# 토크나이저 저장\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "print(f\"토크나이저가 {tokenizer_save_path} 경로에 저장되었습니다.\")\n",
    "\n",
    "# 최종 결과 요약 출력\n",
    "mean_precision = results_df['Precision'].mean()\n",
    "mean_recall = results_df['Recall'].mean()\n",
    "mean_f1 = results_df['F1-Score'].mean()\n",
    "mean_accuracy = results_df['Accuracy'].mean()\n",
    "\n",
    "print(f\"평균 정밀도: {mean_precision:.4f}\")\n",
    "print(f\"평균 재현율: {mean_recall:.4f}\")\n",
    "print(f\"평균 F1 점수: {mean_f1:.4f}\")\n",
    "print(f\"평균 정확도: {mean_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
